---
title: "Blaker's Method (N Unknown)"
author: "Rachel Roggenkemper"
format: html
editor: visual
---

```{r}
library(extraDistr)
library(tidyverse)

source('../functions.R', encoding = 'UTF-8')
```

## Acceptance Curves (Function)

```{r}
# When finding all x's with min tail prob as small, including the fixed_x 
blaker_ac_N_unkown <- function(M, m, conf_level = 0.95, max_N = 250) {
  alpha = 1 - conf_level 
  
  # Initializing data frame to store info through iterations
  results = data.frame(N = integer(), 
                       x = integer(), 
                       min_tail_prob = numeric(), 
                       acceptance_set = character())
  
  # Calculates min_tail_prob for each N and x
  # Iterating through each N
  for (N in M:max_N) {
    # Iterating through each x 
    for (x in 0:(N - M)) {
      # Calculates min_tail_prob
      area_left = ngh_cdf(x = x, N = N, M = M, m = m, lower_tail = TRUE)
      area_right = ngh_cdf(x = x - 1, N = N, M = M, m = m, lower_tail = FALSE)
      min_tail_prob = min(area_left, area_right)
      
      # Stores min_tail_prob in a dataframe along with coresponding M and x
      results = rbind(results, data.frame(N = N, 
                                          x = x,
                                          min_tail_prob = min_tail_prob,
                                          acceptance_set = NA))
    }
  }
  
  # Initializes data frame for output 
  final_results = data.frame(N = integer(), 
                             acceptance_set = character(), 
                             a = integer(), 
                             b = integer(), 
                             cardinality = integer(), 
                             coverage_prob = numeric(), 
                             x_set = character(), 
                             gap = logical())
  
  # Calculate the acceptance sets based on min_tail_prob
  # Iterating through each N
  for (fixed_N in M:max_N) {
    acceptance_set = c()
    x_set = ""

    # Iterating through each x 
    for (fixed_x in 0:(fixed_N - M)) {
      # Finds the min_tail_prob for the fixed_N and fixed_x
      min_tail_prob_fixed_x = results %>% 
        filter(N == fixed_N & x == fixed_x) %>% 
        select(min_tail_prob) %>% 
        pull()
      
      # Finds all x's with min_tail_prob as small as the fixed_x (includes fixed_x)
      possible_xs = results %>% 
        filter(N == fixed_N & min_tail_prob <= min_tail_prob_fixed_x) %>% 
        select(x) %>% 
        pull()
      
      # Sums up the pmf of all x's with min-tail prob as small as fixed_x
      prob_sum = sum(unlist(lapply(possible_xs, function(px) ngh_pmf(x = px, 
                                                                     N = fixed_N, 
                                                                     M = M, 
                                                                     m = m))))
      
      # If the sum of pmf is greater than alpha, add the fixed_x to the acceptance set 
      if (prob_sum > alpha) {
        acceptance_set = c(acceptance_set, fixed_x)
      }
    }
  
    acceptance_set = as.numeric(acceptance_set)
    acceptance_set_str = paste(unique(acceptance_set), collapse = ",")
    
    # Calculating cardinality and coverage probability 
    a = min(acceptance_set)
    b = max(acceptance_set)
    cardinality = length(acceptance_set)
    coverage_prob = sum(unlist(lapply(acceptance_set, function(px) ngh_pmf(x = px, 
                                                                           N = fixed_N, 
                                                                           M = M, 
                                                                           m = m))))
    # gap is a boolean (TRUE / FALSE) of whether there is a gap present in acceptance set
    gap = any(diff(sort(acceptance_set)) > 1)
    
    # If there is no gap, formatting x_set
    if (!gap) {
      x_set = paste(a, b, sep = "-")
    } 
    # When there is a gap, formatting x_set 
    else {
      intervals = c()
      start = acceptance_set[1]
      for (i in 2:length(acceptance_set)) {
        if (acceptance_set[i] != acceptance_set[i - 1] + 1) {
          intervals = c(intervals, paste(start, acceptance_set[i - 1], sep = "-"))
          start = acceptance_set[i]
        }
      }
      intervals = c(intervals, 
                    paste(start, acceptance_set[length(acceptance_set)], sep = "-"))
      x_set = paste(intervals, collapse = ", ")
    }
    
    # Adding final_results to the data frame to be outputted 
    final_results = rbind(final_results, 
                          data.frame(N = fixed_N, 
                                     acceptance_set = acceptance_set_str, 
                                     a = a, 
                                     b = b, 
                                     cardinality = cardinality, 
                                     coverage_prob = coverage_prob, 
                                     x_set = x_set, 
                                     gap = gap))
  }
  final_results = final_results %>%
    arrange(N)
  
  return(final_results)
}
```

```{r}
blaker_ac_N_unkown_vec <- function(M, m, conf_level = 0.95, max_N = 250) {
  alpha <- 1 - conf_level
  
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # 1) FIRST PASS: Build a data frame 'results' with:
  #    (N, x, min_tail_prob, pmf_x)
  #    Instead of looping repeatedly with rbind, we:
  #    - Create combinations for N in [M, max_N] and x in [0, N - M].
  #    - Compute min_tail_prob and pmf_x in a vectorized fashion.
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  N_values <- seq.int(M, max_N)
  
  # Build all (N, x) pairs in one go
  combos <- do.call(
    rbind,
    lapply(N_values, function(N_val) {
      data.frame(
        N = N_val,
        x = seq.int(0, N_val - M)
      )
    })
  )
  
  # Compute min_tail_prob for each (N, x) pair
  # and also store pmf_x to avoid repeated calls later.
  # We'll do this in two steps for clarity.
  combos <- combos %>%
    rowwise() %>%
    mutate(
      area_left  = ngh_cdf(x, N, M, m, lower_tail = TRUE),
      area_right = ngh_cdf(x - 1, N, M, m, lower_tail = FALSE),
      min_tail_prob = min(area_left, area_right),
      pmf_x         = ngh_pmf(x, N, M, m)
    ) %>%
    ungroup()
  
  # We'll keep the "acceptance_set" column as NA here (to match your original structure)
  # though it's not used until the second pass.
  results <- combos %>%
    mutate(acceptance_set = NA_character_) %>%
    select(N, x, min_tail_prob, pmf_x, acceptance_set)
  
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # 2) SECOND PASS: Build 'final_results' by scanning each fixed N
  #    and determining acceptance sets. The logic is unchanged.
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
  final_results <- data.frame(
    N              = integer(),
    acceptance_set = character(),
    a              = integer(),
    b              = integer(),
    cardinality    = integer(),
    coverage_prob  = numeric(),
    x_set          = character(),
    gap            = logical()
  )
  
  for (fixed_N in seq.int(M, max_N)) {
    # Acceptance set for this N
    acceptance_set <- integer(0)
    
    # Subset results for this N just once
    sub_df <- results %>% filter(N == fixed_N)
    
    # For each x in [0, fixed_N - M], check the condition
    x_seq <- seq.int(0, fixed_N - M)
    for (fixed_x in x_seq) {
      # min_tail_prob for x
      min_tp_fx <- sub_df$min_tail_prob[sub_df$x == fixed_x]
      
      # All x's with min_tail_prob <= min_tail_prob_fixed_x
      possible_xs <- sub_df$x[sub_df$min_tail_prob <= min_tp_fx]
      
      # Sum up the pmf of all these x's
      pmf_sum <- sum(sub_df$pmf_x[sub_df$x %in% possible_xs])
      
      # If sum of pmf is > alpha, include fixed_x in acceptance set
      if (pmf_sum > alpha) {
        acceptance_set <- c(acceptance_set, fixed_x)
      }
    }
    
    # Convert to numeric (unique), build acceptance_set string
    acceptance_set <- sort(unique(as.numeric(acceptance_set)))
    acceptance_set_str <- paste(acceptance_set, collapse = ",")
    
    # a, b, cardinality
    a_val         <- min(acceptance_set)
    b_val         <- max(acceptance_set)
    cardinality   <- length(acceptance_set)
    
    # Coverage probability: sum pmf_x for x in acceptance_set
    coverage_prob <- sum(sub_df$pmf_x[sub_df$x %in% acceptance_set])
    
    # Check if there is a gap
    gap_val <- any(diff(acceptance_set) > 1)
    
    # Build x_set (interval notation)
    x_set_str <- ""
    if (!gap_val) {
      # No gap => single interval
      x_set_str <- paste(a_val, b_val, sep = "-")
    } else {
      # Gap => multiple intervals
      intervals <- c()
      start_int <- acceptance_set[1]
      for (i in seq.int(2, length(acceptance_set))) {
        if (acceptance_set[i] != acceptance_set[i - 1] + 1) {
          intervals <- c(intervals, paste(start_int, acceptance_set[i - 1], sep = "-"))
          start_int <- acceptance_set[i]
        }
      }
      # Add last interval
      intervals <- c(intervals, paste(start_int, acceptance_set[length(acceptance_set)], sep = "-"))
      x_set_str <- paste(intervals, collapse = ", ")
    }
    
    # Add row to final_results
    final_results <- rbind(
      final_results,
      data.frame(
        N               = fixed_N,
        acceptance_set  = acceptance_set_str,
        a               = a_val,
        b               = b_val,
        cardinality     = cardinality,
        coverage_prob   = coverage_prob,
        x_set           = x_set_str,
        gap             = gap_val
      )
    )
  }
  
  final_results <- final_results %>%
    arrange(N)
  
  return(final_results)
}
```

```{r}
blaker_ac_N_unkown(M = 10, m = 5, conf_level = 0.95, max_N = 40)
```

```{r}
blaker_ac_N_unkown_vec(M = 10, m = 5, conf_level = 0.95, max_N = 40)
```

## Confidence Intervals Bounds (Function)

```{r}
blaker_ci_N_unkown <- function(M, m, conf_level = 0.95, max_N = 250, procedure = "MST") {
  results = blaker_ac_N_unkown(M, m, conf_level)
  
  # Check if there are any gaps
  if (any(results$gap)) {
    return("Gaps present in acceptance sets")
  }
  
  # Initializes data frame that will be outputted 
  ci_results = data.frame(x = integer(), 
                          ci_lb = integer(), 
                          ci_ub = integer(), 
                          ci = character(),
                          stringsAsFactors = FALSE)
  
  # Find the second-highest "a"
  unique_a_values <- sort(unique(results$a), decreasing = TRUE)
  if (length(unique_a_values) > 1) {
    max_x <- unique_a_values[2]  # Second-highest value
  } else {
    max_x <- unique_a_values[1]  # Fallback if only one value exists
  }
  
  # Loops through each x 
  for (x in 0:max_x) {
    # Finds first interval where x appears 
    first_occurrence = results %>% 
      filter(a <= x, x <= b) %>% 
      slice(1)
    
    # Finds last interval where x appears 
    last_occurrence = results %>% 
      filter(a <= x, x <= b) %>% 
      slice(n())
    
    # Finds the N of the corresponding above intervals and saves those as the CI bounds
    if (nrow(first_occurrence) > 0 && nrow(last_occurrence) > 0) {
      ci_lb = first_occurrence$N
      ci_ub = last_occurrence$N
      ci = paste0("[", ci_lb, ", ", ci_ub, "]")
      
      ci_results = rbind(ci_results, data.frame(x = x, 
                                                ci_lb = ci_lb, 
                                                ci_ub = ci_ub, 
                                                ci = ci))
    }
  }
  
  return(ci_results)
}
```

```{r}
blaker_ci_N_unkown_vec <- function(M, m, conf_level = 0.95, max_N = 250, procedure = "MST") {
  results <- blaker_ac_N_unkown_vec(M, m, conf_level)
  
  # If there's any gap, just return
  if (any(results$gap)) {
    return("Gaps present in acceptance sets")
  }
  
  unique_a_values <- sort(unique(results$a), decreasing = TRUE)
  max_x <- if (length(unique_a_values) > 1) unique_a_values[2] else unique_a_values[1]
  
  x_values <- seq.int(0, max_x)
  row_list <- vector("list", length(x_values))
  
  for (i in seq_along(x_values)) {
    x_val <- x_values[i]
    
    first_occurrence <- results %>% filter(a <= x_val, x_val <= b) %>% slice(1)
    last_occurrence  <- results %>% filter(a <= x_val, x_val <= b) %>% slice(n())
    
    if (nrow(first_occurrence) > 0 && nrow(last_occurrence) > 0) {
      ci_lb <- first_occurrence$N
      ci_ub <- last_occurrence$N
      ci_str <- paste0("[", ci_lb, ", ", ci_ub, "]")
      
      row_list[[i]] <- data.frame(
        x = x_val,
        ci_lb = ci_lb,
        ci_ub = ci_ub,
        ci = ci_str,
        stringsAsFactors = FALSE
      )
    }
  }
  
  ci_results <- do.call(rbind, row_list[!sapply(row_list, is.null)])
  return(ci_results)
}
```

```{r}
# blaker_ci_N_unkown(M = 10, m = 5, conf_level = 0.95, max_N = 60)

# blaker_ci_N_unkown_vec(M = 10, m = 5, conf_level = 0.95, max_N = 60)
```

## Coverage Probability Plot

```{r}
# cov_prob_blaker = blaker_ac_N_unkown(M = 10, m = 5, conf_level = 0.95, max_N = 250)
# write.csv(cov_prob_blaker, file.path("/Users/rachelroggenkemper/Documents/NegativeHypergeometric/N_Unknown", "cov_prob_blaker.csv"), row.names = FALSE)

cov_prob_blaker = read_csv("cov_prob_blaker.csv")

conf_level= 0.95

ggplot(cov_prob_blaker, aes(x = N, y = coverage_prob, group = x_set, color = x_set)) +
  geom_line() +
  geom_point() +
  labs(x = "N", y = "Coverage Probability of N") +
  geom_hline(yintercept = conf_level, color = "red") +
  ylim(conf_level - 0.005, 1) +
  theme_minimal() +
  theme(legend.position = "none")
```
